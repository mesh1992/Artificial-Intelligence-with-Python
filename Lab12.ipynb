{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab12.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMP8vZlnfzEJRDcvNFGh/mV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mesh1992/Artificial-Intelligence-with-Python/blob/master/Lab12.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9F4k56rDGRW4"
      },
      "outputs": [],
      "source": [
        "#1.\tGradient boosting classifier\n",
        "\n",
        "from sklearn.datasets import load_iris \n",
        "from sklearn.model_selection import train_test_split \n",
        "from sklearn.ensemble import GradientBoostingClassifier \n",
        "iris = load_iris()\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=42)\n",
        "gbrt = GradientBoostingClassifier(learning_rate=0.01, random_state=0)\n",
        "\n",
        "gbrt.fit(X_train, y_train)\n",
        "\n",
        "print('The decision function for the 3-class iris dataset:\\n\\n{}'.format(gbrt.decision_function(X_test[:10])))\n",
        "\n",
        "print('Predicted probabilities for the samples in the iris dataset:\\n\\n{}'.format(gbrt.predict_proba(X_test[:10])))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2.\tPreprocessing\n",
        "\n",
        "from sklearn import preprocessing \n",
        "import numpy as np \n",
        "\n",
        "data = np.array([[2.2, 5.9, -1.8], [5.4, -3.2, -5.1], [-1.9, 4.2, 3.2]])\n",
        "bindata = preprocessing.Binarizer(threshold=1.5).transform(data)\n",
        "print('Binarized data:\\n\\n', bindata)\n",
        "\n",
        "print('Mean (before)= ', data.mean(axis=0))\n",
        "print('Standard Deviation (before)= ', data.std(axis=0))\n",
        "scaled_data = preprocessing.scale(data)\n",
        "\n",
        "print('Mean (after)= ', scaled_data.mean(axis=0))\n",
        "print('Standard Deviation (after)= ', scaled_data.std(axis=0))\n",
        "\n",
        "data\n",
        "minmax_scaler = preprocessing.MinMaxScaler(feature_range=(0,1))\n",
        "data_minmax = minmax_scaler.fit_transform(data)\n",
        "print('MinMaxScaler applied on the data:\\n', data_minmax)\n",
        "\n",
        "data\n",
        "data_l1 = preprocessing.normalize(data, norm='l1')\n",
        "data_l2 = preprocessing.normalize(data, norm='l2')\n",
        "\n",
        "print('L1-normalized data:\\n', data_l1)\n",
        "print('\\nL2-normalized data:\\n', data_l2)\n",
        "\n"
      ],
      "metadata": {
        "id": "M0flUU8AGs53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3.\tLabel encoding \n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "\n",
        "data = pd.read_csv('adult_data.txt', header=None, index_col=False, names=['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', 'relationship', 'race', 'gender', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'income'])\n",
        "\n",
        "data = data[['age', 'workclass', 'education', 'gender', 'hours-per-week', 'occupation', 'income']]\n",
        "display(data)\n",
        "\n",
        "\n",
        "print('Original Features:\\n', list(data.columns), '\\n') \n",
        "data_dummies = pd.get_dummies(data) \n",
        "print('Features after One-Hot Encoding:\\n', list(data_dummies.columns))\n",
        "\n",
        "features = data_dummies.loc[:, 'age':'occupation_ Transport-moving']\n",
        "X = features.values\n",
        "y = data_dummies['income_ >50K'].values\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
        "logreg = LogisticRegression()\n",
        "logreg.fit(X_train, y_train)\n",
        "\n",
        "print('Logistic Regression score on the test set: {:.2f}'.format(logreg.score(X_test, y_test)))\n"
      ],
      "metadata": {
        "id": "q7dkoCfRGxLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4.\tAutomatic Feature Selection\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_selection import SelectPercentile\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "%matplotlib inline\n",
        "\n",
        "cancer = load_breast_cancer()\n",
        "\n",
        "rng = np.random.RandomState(42)\n",
        "noise = rng.normal(size=(len(cancer.data), 50))\n",
        "\n",
        "X_w_noise = np.hstack([cancer.data, noise])\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_w_noise, cancer.target, random_state=0, test_size=.5)\n",
        "\n",
        "select = SelectPercentile(percentile=50)\n",
        "select.fit(X_train, y_train)\n",
        "X_train_selected = select.transform(X_train)\n",
        "\n",
        "print('X_train.shape is: {}'.format(X_train.shape))\n",
        "print('X_train_selected.shape is: {}'.format(X_train_selected.shape))\n",
        "\n",
        "\n",
        "mask = select.get_support() \n",
        "print(mask) \n",
        "plt.matshow(mask.reshape(1,-1), cmap='gray_r')\n",
        "\n",
        "\n",
        "X_test_selected = select.transform(X_test) \n",
        "logreg = LogisticRegression() \n",
        "logreg.fit(X_train, y_train) \n",
        "print('The score of Logistic Regression on all features: {:.3f}'.format(logreg.score(X_test, y_test))) \n",
        "logreg.fit(X_train_selected, y_train) \n",
        "print('The score of Logistic Regression on the selected features: {:.3f}'.format(logreg.score(X_test_selected, y_test)))\n",
        "\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "select = SelectFromModel(RandomForestClassifier(n_estimators=100, random_state=42), threshold='median')\n",
        "\n",
        "select.fit(X_train, y_train)\n",
        "X_train_s = select.transform(X_train)\n",
        "print('The shape of X_train is: ', X_train.shape)\n",
        "print('The shape of X_train_s is ', X_train_s.shape)\n",
        "\n",
        "mask = select.get_support()\n",
        "plt.matshow(mask.reshape(1,-1), cmap='gray_r')\n",
        "plt.xlabel('Index of Features')\n",
        "\n",
        "X_test_s = select.transform(X_test)\n",
        "score = LogisticRegression().fit(X_train_s, y_train).score(X_test_s, y_test)\n",
        "print('The score of Logistic Regression with the selected features on the test set: {:.3f}'.format(score))\n"
      ],
      "metadata": {
        "id": "dk2sZ4oxG1_U"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}